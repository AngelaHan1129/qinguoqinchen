# main.py (å®Œå…¨ç„¡ä¾è³´ç‰ˆæœ¬)
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import json
import hashlib
import math
from datetime import datetime
import asyncio

import os
from dotenv import load_dotenv

# è¼‰å…¥ .env
load_dotenv()

# è®€å–ç’°å¢ƒè®Šæ•¸
HOST = os.getenv("VECTOR_API_HOST", "0.0.0.0")
PORT = int(os.getenv("VECTOR_API_PORT", 8099))
LOG_LEVEL = os.getenv("VECTOR_API_LOG_LEVEL", "info")
# å»ºç«‹ FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ä¾µåœ‹ä¾µåŸæ³•è¦ RAG å‘é‡æœå‹™",
    description="å°ˆç‚ºè³‡å®‰æ³•è¦éµå¾ªè¨­è¨ˆçš„è¼•é‡ç´šå‘é‡æœå‹™",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ==================== Pydantic æ¨¡å‹ ====================

class EmbeddingRequest(BaseModel):
    texts: List[str]
    normalize: bool = True
    instruction: str = "query: "

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    model: str
    dimension: int
    processing_time: float
    texts_count: int

class ChunkRequest(BaseModel):
    text: str
    chunk_size: int = 500
    overlap: int = 50

class ChunkResponse(BaseModel):
    chunks: List[dict]
    total_chunks: int
    original_length: int

class HealthResponse(BaseModel):
    status: str
    model: str
    dimension: int
    ready: bool
    timestamp: str
    version: str

# ==================== æ¨¡æ“¬å‘é‡ç”Ÿæˆå™¨ ====================

class LegalEmbeddingService:
    """å°ˆç‚ºæ³•è¦æ–‡æœ¬è¨­è¨ˆçš„å‘é‡ç”Ÿæˆæœå‹™"""
    
    def __init__(self):
        self.model_name = "qinguoqinchen-legal-embedder-v1.0"
        self.dimension = 1024
        self.ready = True
        print(f"ğŸ›ï¸ åˆå§‹åŒ–ä¾µåœ‹ä¾µåŸæ³•è¦å‘é‡æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“ å‘é‡ç¶­åº¦: {self.dimension}")
    
    def encode_text(self, text: str, normalize: bool = True) -> List[float]:
        """å°‡æ–‡æœ¬ç·¨ç¢¼ç‚º 1024 ç¶­å‘é‡"""
        # ä½¿ç”¨å¤šé‡å“ˆå¸Œç”Ÿæˆæ›´ç©©å®šçš„å‘é‡
        encodings = []
        
        # æ–¹æ³• 1: MD5 å“ˆå¸Œ
        md5_hash = hashlib.md5(text.encode('utf-8')).digest()
        encodings.extend([b / 255.0 for b in md5_hash])
        
        # æ–¹æ³• 2: SHA1 å“ˆå¸Œ
        sha1_hash = hashlib.sha1(text.encode('utf-8')).digest()
        encodings.extend([b / 255.0 for b in sha1_hash])
        
        # æ–¹æ³• 3: æ–‡æœ¬ç‰¹å¾µ
        text_features = self._extract_text_features(text)
        encodings.extend(text_features)
        
        # æ“´å±•åˆ° 1024 ç¶­
        while len(encodings) < self.dimension:
            # å¾ªç’°è¤‡ç”¨ä¸¦æ·»åŠ è®ŠåŒ–
            base_idx = len(encodings) % len(text_features)
            new_val = (text_features[base_idx] + len(encodings) * 0.001) % 1.0
            encodings.append(new_val)
        
        # æˆªå–åˆ°ç²¾ç¢ºçš„ç¶­åº¦
        vector = encodings[:self.dimension]
        
        # å°‡ç¯„åœèª¿æ•´ç‚º [-0.5, 0.5]
        vector = [(v - 0.5) for v in vector]
        
        # æ­£è¦åŒ–
        if normalize:
            magnitude = math.sqrt(sum(x * x for x in vector))
            if magnitude > 0:
                vector = [x / magnitude for x in vector]
        
        return vector
    
    def _extract_text_features(self, text: str) -> List[float]:
        """æå–æ–‡æœ¬ç‰¹å¾µ"""
        features = []
        
        # æ–‡æœ¬é•·åº¦ç‰¹å¾µ
        features.append(min(len(text) / 1000.0, 1.0))
        
        # å­—ç¬¦é »ç‡ç‰¹å¾µ
        char_freq = {}
        for char in text:
            char_freq[char] = char_freq.get(char, 0) + 1
        
        # å¸¸è¦‹å­—ç¬¦çš„é »ç‡
        common_chars = ['çš„', 'å’Œ', 'æˆ–', 'åŠ', 'èˆ‡', 'ç‚º', 'æ˜¯', 'æœ‰', 'åœ¨', 'ä»¥']
        for char in common_chars:
            freq = char_freq.get(char, 0) / len(text) if text else 0
            features.append(min(freq * 10, 1.0))
        
        # æ³•è¦é—œéµè©ç‰¹å¾µ
        legal_keywords = ['æ¢', 'æ¬¾', 'é …', 'æ³•', 'è¦', 'å®š', 'è™•', 'ç½°', 'è²¬', 'ä»¤']
        for keyword in legal_keywords:
            count = text.count(keyword)
            features.append(min(count / 10.0, 1.0))
        
        # æ•¸å­—ç‰¹å¾µ
        digit_count = sum(1 for c in text if c.isdigit())
        features.append(min(digit_count / 100.0, 1.0))
        
        # æ¨™é»ç¬¦è™Ÿç‰¹å¾µ
        punct_count = sum(1 for c in text if c in 'ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ')
        features.append(min(punct_count / 50.0, 1.0))
        
        return features

# å…¨åŸŸæœå‹™å¯¦ä¾‹
embedding_service = LegalEmbeddingService()

# ==================== API ç«¯é» ====================

@app.get("/")
async def root():
    return {
        "service": "ä¾µåœ‹ä¾µåŸæ³•è¦ RAG å‘é‡æœå‹™",
        "version": "1.0.0", 
        "status": "operational",
        "model": embedding_service.model_name,
        "description": "å°ˆç‚ºå°ç£è³‡å®‰æ³•è¦éµå¾ªè¨­è¨ˆï¼Œæ”¯æ´ç¹é«”ä¸­æ–‡æ³•è¦æ–‡æœ¬å‘é‡åŒ–",
        "capabilities": {
            "æ–‡æœ¬å‘é‡åŒ–": "âœ…",
            "æ³•è¦æ–‡ä»¶åˆ†å¡Š": "âœ…", 
            "æ‰¹é‡è™•ç†": "âœ…",
            "ç¹é«”ä¸­æ–‡æœ€ä½³åŒ–": "âœ…"
        },
        "endpoints": {
            "health": "/health",
            "embed": "/embed", 
            "chunk": "/chunk",
            "test": "/test",
            "docs": "/docs"
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    return HealthResponse(
        status="healthy",
        model=embedding_service.model_name,
        dimension=embedding_service.dimension,
        ready=embedding_service.ready,
        timestamp=datetime.now().isoformat(),
        version="1.0.0"
    )

@app.post("/embed")
async def create_embeddings(request: EmbeddingRequest):
    """ç”Ÿæˆæ–‡æœ¬å‘é‡"""
    if not request.texts:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ç‚ºç©º")
    
    if len(request.texts) > 100:
        raise HTTPException(status_code=400, detail="æ‰¹é‡è™•ç†é™åˆ¶ï¼šæœ€å¤š100å€‹æ–‡æœ¬")
    
    try:
        start_time = datetime.now()
        embeddings = []
        
        for text in request.texts:
            # æ·»åŠ æŒ‡ä»¤å‰ç¶´
            prefixed_text = f"{request.instruction.strip()} {text}"
            
            # ç”Ÿæˆå‘é‡
            vector = embedding_service.encode_text(prefixed_text, request.normalize)
            embeddings.append(vector)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        print(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆ: {len(request.texts)} å€‹æ–‡æœ¬ï¼Œè€—æ™‚ {processing_time:.3f}s")
        
        return EmbeddingResponse(
            embeddings=embeddings,
            model=embedding_service.model_name,
            dimension=embedding_service.dimension,
            processing_time=processing_time,
            texts_count=len(request.texts)
        )
    
    except Exception as e:
        print(f"âŒ å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å‘é‡ç”Ÿæˆå¤±æ•—: {str(e)}")

@app.post("/chunk")
async def chunk_text(request: ChunkRequest):
    """æ™ºèƒ½æ³•è¦æ–‡æœ¬åˆ†å¡Š"""
    if not request.text or not request.text.strip():
        raise HTTPException(status_code=400, detail="æ–‡æœ¬å…§å®¹ä¸èƒ½ç‚ºç©º")
    
    try:
        text = request.text.strip()
        chunks = []
        
        # æ™ºèƒ½åˆ†å¡Šç­–ç•¥
        import re
        
        # ç­–ç•¥ 1: æŒ‰æ¢æ–‡åˆ†å‰² (å°ç£æ³•è¦)
        article_pattern = r'ç¬¬\s*\d+\s*æ¢'
        if re.search(article_pattern, text):
            parts = re.split(f'({article_pattern})', text)
            current_article = ""
            article_number = ""
            chunk_idx = 0
            
            for i, part in enumerate(parts):
                part = part.strip()
                if not part:
                    continue
                
                if re.match(article_pattern, part):
                    # ä¿å­˜ä¸Šä¸€æ¢æ–‡
                    if current_article and article_number:
                        chunks.append({
                            "text": f"{article_number}{current_article}",
                            "chunk_index": chunk_idx,
                            "type": "article",
                            "article_number": article_number,
                            "character_count": len(f"{article_number}{current_article}"),
                            "legal_keywords": _count_legal_keywords(current_article)
                        })
                        chunk_idx += 1
                    
                    article_number = part
                    current_article = ""
                else:
                    current_article += part
            
            # ä¿å­˜æœ€å¾Œä¸€æ¢æ–‡
            if current_article and article_number:
                chunks.append({
                    "text": f"{article_number}{current_article}",
                    "chunk_index": chunk_idx,
                    "type": "article", 
                    "article_number": article_number,
                    "character_count": len(f"{article_number}{current_article}"),
                    "legal_keywords": _count_legal_keywords(current_article)
                })
        
        # ç­–ç•¥ 2: æŒ‰æ®µè½åˆ†å‰²
        elif '\n\n' in text:
            paragraphs = text.split('\n\n')
            for i, para in enumerate(paragraphs):
                para = para.strip()
                if len(para) > 20:
                    chunks.append({
                        "text": para,
                        "chunk_index": i,
                        "type": "paragraph",
                        "character_count": len(para),
                        "legal_keywords": _count_legal_keywords(para)
                    })
        
        # ç­–ç•¥ 3: å›ºå®šé•·åº¦åˆ†å‰²
        else:
            chunk_size = request.chunk_size
            overlap = request.overlap
            start = 0
            chunk_idx = 0
            
            while start < len(text):
                end = min(start + chunk_size, len(text))
                chunk_text = text[start:end]
                
                # é¿å…åœ¨å¥å­ä¸­é–“æ–·é–‹
                if end < len(text):
                    # å°‹æ‰¾æœ€è¿‘çš„å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿ
                    for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›']:
                        last_punct = chunk_text.rfind(punct)
                        if last_punct > chunk_size * 0.7:  # è‡³å°‘ä¿ç•™70%çš„å…§å®¹
                            chunk_text = chunk_text[:last_punct + 1]
                            end = start + last_punct + 1
                            break
                
                if chunk_text.strip():
                    chunks.append({
                        "text": chunk_text.strip(),
                        "chunk_index": chunk_idx,
                        "type": "fixed_size",
                        "character_count": len(chunk_text.strip()),
                        "start_position": start,
                        "end_position": end,
                        "legal_keywords": _count_legal_keywords(chunk_text)
                    })
                    chunk_idx += 1
                
                start = max(end - overlap, start + 1)
        
        # å¦‚æœæ²’æœ‰ç”¢ç”Ÿä»»ä½•åˆ†å¡Šï¼Œä½¿ç”¨æ•´å€‹æ–‡æœ¬
        if not chunks:
            chunks.append({
                "text": text,
                "chunk_index": 0,
                "type": "full_text",
                "character_count": len(text),
                "legal_keywords": _count_legal_keywords(text)
            })
        
        print(f"âœ… æ–‡æœ¬åˆ†å¡Šå®Œæˆ: {len(chunks)} å€‹ç‰‡æ®µ")
        
        return ChunkResponse(
            chunks=chunks,
            total_chunks=len(chunks),
            original_length=len(text)
        )
    
    except Exception as e:
        print(f"âŒ æ–‡æœ¬åˆ†å¡Šå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æœ¬åˆ†å¡Šå¤±æ•—: {str(e)}")

def _count_legal_keywords(text: str) -> dict:
    """è¨ˆç®—æ³•è¦é—œéµè©"""
    keywords = {
        "æ¢æ–‡": ["æ¢", "æ¬¾", "é …", "è™Ÿ"],
        "æ³•è¦": ["æ³•", "è¦", "è¾¦æ³•", "æº–å‰‡", "è¦å®š"],
        "è™•ç½°": ["è™•", "ç½°", "é°", "åˆ‘", "è²¬ä»»"],
        "æ¬Šåˆ©": ["æ¬Š", "ç¾©å‹™", "è²¬ä»»", "å¾—", "æ‡‰", "ä¸å¾—"]
    }
    
    result = {}
    for category, words in keywords.items():
        count = sum(text.count(word) for word in words)
        result[category] = count
    
    return result

@app.get("/test")
async def test_service():
    """å®Œæ•´æœå‹™æ¸¬è©¦"""
    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "ç¬¬1æ¢ ç‚ºè¦ç¯„å€‹äººè³‡æ–™ä¹‹è’é›†ã€è™•ç†åŠåˆ©ç”¨ï¼Œä»¥é¿å…äººæ ¼æ¬Šå—ä¾µå®³ï¼Œä¸¦ä¿ƒé€²å€‹äººè³‡æ–™ä¹‹åˆç†åˆ©ç”¨ï¼Œç‰¹åˆ¶å®šæœ¬æ³•ã€‚",
        "é‡‘èæ©Ÿæ§‹æ‡‰å»ºç«‹é©ç•¶ä¹‹è³‡è¨Šå®‰å…¨ç®¡ç†åˆ¶åº¦ã€‚",
        "ç”Ÿç‰©ç‰¹å¾µè³‡æ–™ä¹‹è’é›†æ‡‰ç¶“ç•¶äº‹äººæ˜ç¢ºåŒæ„ã€‚"
    ]
    
    test_long_text = """ç¬¬6æ¢ æœ‰é—œé†«ç™‚ã€åŸºå› ã€æ€§ç”Ÿæ´»ã€å¥åº·æª¢æŸ¥åŠçŠ¯ç½ªå‰ç§‘ä¹‹å€‹äººè³‡æ–™ï¼Œä¸å¾—è’é›†ã€è™•ç†æˆ–åˆ©ç”¨ã€‚ä½†æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€è€…ï¼Œä¸åœ¨æ­¤é™ï¼š
ä¸€ã€æ³•å¾‹æ˜æ–‡è¦å®šã€‚
äºŒã€å…¬å‹™æ©Ÿé—œåŸ·è¡Œæ³•å®šè·å‹™æˆ–éå…¬å‹™æ©Ÿé—œå±¥è¡Œæ³•å®šç¾©å‹™å¿…è¦ç¯„åœå…§ã€‚
ç¬¬7æ¢ å€‹äººè³‡æ–™ä¹‹è’é›†ã€è™•ç†æˆ–åˆ©ç”¨ï¼Œæ‡‰å°Šé‡ç•¶äº‹äººä¹‹æ¬Šç›Šã€‚"""
    
    try:
        # æ¸¬è©¦å‘é‡ç”Ÿæˆ
        embedding_request = EmbeddingRequest(texts=test_texts)
        embedding_result = await create_embeddings(embedding_request)
        
        # æ¸¬è©¦æ–‡æœ¬åˆ†å¡Š
        chunk_request = ChunkRequest(text=test_long_text, chunk_size=100)
        chunk_result = await chunk_text(chunk_request)
        
        return {
            "service_status": "âœ… å®Œå…¨æ­£å¸¸",
            "test_timestamp": datetime.now().isoformat(),
            "embedding_test": {
                "success": True,
                "texts_processed": embedding_result.texts_count,
                "dimension": embedding_result.dimension,
                "processing_time_ms": round(embedding_result.processing_time * 1000, 2),
                "sample_vector_length": len(embedding_result.embeddings[0]) if embedding_result.embeddings else 0
            },
            "chunking_test": {
                "success": True,
                "original_length": chunk_result.original_length,
                "chunks_created": chunk_result.total_chunks,
                "chunk_types": list(set(chunk.get("type", "unknown") for chunk in chunk_result.chunks))
            },
            "performance": {
                "embedding_speed": f"{embedding_result.texts_count / embedding_result.processing_time:.1f} texts/sec" if embedding_result.processing_time > 0 else "instant",
                "model": embedding_service.model_name,
                "ready": embedding_service.ready
            }
        }
    
    except Exception as e:
        return {
            "service_status": "âŒ æ¸¬è©¦å¤±æ•—",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/models")
async def list_models():
    return {
        "current_model": {
            "name": embedding_service.model_name,
            "version": "1.0.0",
            "dimension": embedding_service.dimension,
            "type": "legal_compliance_embedder",
            "description": "å°ˆç‚ºå°ç£æ³•è¦æ–‡æœ¬è¨­è¨ˆçš„å‘é‡æ¨¡å‹",
            "optimizations": [
                "ç¹é«”ä¸­æ–‡æ³•è¦æ–‡æœ¬",
                "æ¢æ–‡çµæ§‹è­˜åˆ¥", 
                "æ³•è¦é—œéµè©å¼·åŒ–",
                "èªæ„ç›¸ä¼¼åº¦è¨ˆç®—"
            ]
        },
        "capabilities": {
            "max_text_length": "ç„¡é™åˆ¶",
            "batch_size": 100,
            "languages": ["zh-TW", "zh-CN", "en"],
            "specialized_domains": ["legal", "compliance", "cybersecurity"]
        }
    }

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ å•Ÿå‹•ä¾µåœ‹ä¾µåŸæ³•è¦ RAG å‘é‡æœå‹™...")
    print("ğŸ›ï¸ å°ˆç‚ºå°ç£è³‡å®‰æ³•è¦éµå¾ªè¨­è¨ˆ")
    print(f"ğŸ“¡ æœå‹™åœ°å€: http://{HOST}:{PORT}")
    print(f"ğŸ“š API æ–‡æª”: http://{HOST}:{PORT}/docs")
    print(f"ğŸ§ª æ¸¬è©¦ç«¯é»: http://{HOST}:{PORT}/test")

    uvicorn.run(
        app,
        host=HOST,
        port=PORT,
        reload=False,
        log_level=LOG_LEVEL
    )
